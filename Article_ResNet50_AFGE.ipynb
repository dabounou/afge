{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfd920d-6c72-43d6-90b5-88cd26d8e72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vpro/opt/anaconda3/envs/Torch_Info_theory/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/vpro/opt/anaconda3/envs/Torch_Info_theory/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2239 (0%)]\tLoss: 2.349205\n",
      "Train Epoch: 1 [320/2239 (14%)]\tLoss: 1.916934\n",
      "Train Epoch: 1 [640/2239 (29%)]\tLoss: 1.815257\n",
      "Train Epoch: 1 [960/2239 (43%)]\tLoss: 1.590439\n",
      "Train Epoch: 1 [1280/2239 (57%)]\tLoss: 1.466196\n",
      "Train Epoch: 1 [1600/2239 (71%)]\tLoss: 1.563826\n",
      "Train Epoch: 1 [1920/2239 (86%)]\tLoss: 1.583571\n",
      "Epoch 1 training time: 601.51 seconds\n",
      "\n",
      "Test set: Average loss: 1.8563, Accuracy: 39/118 (33.05%)\n",
      "\n",
      "Train Epoch: 2 [0/2239 (0%)]\tLoss: 1.310304\n",
      "Train Epoch: 2 [320/2239 (14%)]\tLoss: 1.527742\n",
      "Train Epoch: 2 [640/2239 (29%)]\tLoss: 1.146703\n",
      "Train Epoch: 2 [960/2239 (43%)]\tLoss: 1.238822\n",
      "Train Epoch: 2 [1280/2239 (57%)]\tLoss: 1.041537\n",
      "Train Epoch: 2 [1600/2239 (71%)]\tLoss: 1.188660\n",
      "Train Epoch: 2 [1920/2239 (86%)]\tLoss: 1.347814\n",
      "Epoch 2 training time: 754.47 seconds\n",
      "\n",
      "Test set: Average loss: 2.0576, Accuracy: 43/118 (36.44%)\n",
      "\n",
      "Train Epoch: 3 [0/2239 (0%)]\tLoss: 1.337140\n",
      "Train Epoch: 3 [320/2239 (14%)]\tLoss: 0.939988\n",
      "Train Epoch: 3 [640/2239 (29%)]\tLoss: 1.303268\n",
      "Train Epoch: 3 [960/2239 (43%)]\tLoss: 1.174134\n",
      "Train Epoch: 3 [1280/2239 (57%)]\tLoss: 1.362497\n",
      "Train Epoch: 3 [1600/2239 (71%)]\tLoss: 1.205459\n",
      "Train Epoch: 3 [1920/2239 (86%)]\tLoss: 1.254535\n",
      "Epoch 3 training time: 761.41 seconds\n",
      "\n",
      "Test set: Average loss: 1.6107, Accuracy: 59/118 (50.00%)\n",
      "\n",
      "Train Epoch: 4 [0/2239 (0%)]\tLoss: 0.992731\n",
      "Train Epoch: 4 [320/2239 (14%)]\tLoss: 1.248494\n",
      "Train Epoch: 4 [640/2239 (29%)]\tLoss: 1.049463\n",
      "Train Epoch: 4 [960/2239 (43%)]\tLoss: 0.988162\n",
      "Train Epoch: 4 [1280/2239 (57%)]\tLoss: 1.093067\n",
      "Train Epoch: 4 [1600/2239 (71%)]\tLoss: 1.599796\n",
      "Train Epoch: 4 [1920/2239 (86%)]\tLoss: 1.381443\n",
      "Epoch 4 training time: 754.09 seconds\n",
      "\n",
      "Test set: Average loss: 1.7179, Accuracy: 57/118 (48.31%)\n",
      "\n",
      "Train Epoch: 5 [0/2239 (0%)]\tLoss: 1.130731\n",
      "Train Epoch: 5 [320/2239 (14%)]\tLoss: 1.103919\n",
      "Train Epoch: 5 [640/2239 (29%)]\tLoss: 1.410896\n",
      "Train Epoch: 5 [960/2239 (43%)]\tLoss: 0.824527\n",
      "Train Epoch: 5 [1280/2239 (57%)]\tLoss: 0.988000\n",
      "Train Epoch: 5 [1600/2239 (71%)]\tLoss: 1.069420\n",
      "Train Epoch: 5 [1920/2239 (86%)]\tLoss: 1.105618\n",
      "Epoch 5 training time: 759.09 seconds\n",
      "\n",
      "Test set: Average loss: 1.5113, Accuracy: 65/118 (55.08%)\n",
      "\n",
      "Train Epoch: 6 [0/2239 (0%)]\tLoss: 1.214893\n",
      "Train Epoch: 6 [320/2239 (14%)]\tLoss: 0.790940\n",
      "Train Epoch: 6 [640/2239 (29%)]\tLoss: 1.020995\n",
      "Train Epoch: 6 [960/2239 (43%)]\tLoss: 0.898993\n",
      "Train Epoch: 6 [1280/2239 (57%)]\tLoss: 1.352822\n",
      "Train Epoch: 6 [1600/2239 (71%)]\tLoss: 1.076122\n",
      "Train Epoch: 6 [1920/2239 (86%)]\tLoss: 1.038538\n",
      "Epoch 6 training time: 757.07 seconds\n",
      "\n",
      "Test set: Average loss: 1.7649, Accuracy: 57/118 (48.31%)\n",
      "\n",
      "Train Epoch: 7 [0/2239 (0%)]\tLoss: 1.426988\n",
      "Train Epoch: 7 [320/2239 (14%)]\tLoss: 0.906426\n",
      "Train Epoch: 7 [640/2239 (29%)]\tLoss: 0.718738\n",
      "Train Epoch: 7 [960/2239 (43%)]\tLoss: 0.998183\n",
      "Train Epoch: 7 [1280/2239 (57%)]\tLoss: 0.703046\n",
      "Train Epoch: 7 [1600/2239 (71%)]\tLoss: 0.974779\n",
      "Train Epoch: 7 [1920/2239 (86%)]\tLoss: 0.658217\n",
      "Epoch 7 training time: 770.21 seconds\n",
      "\n",
      "Test set: Average loss: 1.4517, Accuracy: 65/118 (55.08%)\n",
      "\n",
      "Train Epoch: 8 [0/2239 (0%)]\tLoss: 0.840974\n",
      "Train Epoch: 8 [320/2239 (14%)]\tLoss: 1.029691\n",
      "Train Epoch: 8 [640/2239 (29%)]\tLoss: 1.019154\n",
      "Train Epoch: 8 [960/2239 (43%)]\tLoss: 1.086622\n",
      "Train Epoch: 8 [1280/2239 (57%)]\tLoss: 0.779963\n",
      "Train Epoch: 8 [1600/2239 (71%)]\tLoss: 0.593146\n",
      "Train Epoch: 8 [1920/2239 (86%)]\tLoss: 1.132766\n",
      "Epoch 8 training time: 773.86 seconds\n",
      "\n",
      "Test set: Average loss: 1.4640, Accuracy: 65/118 (55.08%)\n",
      "\n",
      "Train Epoch: 9 [0/2239 (0%)]\tLoss: 0.654413\n",
      "Train Epoch: 9 [320/2239 (14%)]\tLoss: 1.030969\n",
      "Train Epoch: 9 [640/2239 (29%)]\tLoss: 0.987390\n",
      "Train Epoch: 9 [960/2239 (43%)]\tLoss: 0.498044\n",
      "Train Epoch: 9 [1280/2239 (57%)]\tLoss: 0.729252\n",
      "Train Epoch: 9 [1600/2239 (71%)]\tLoss: 1.039485\n",
      "Train Epoch: 9 [1920/2239 (86%)]\tLoss: 0.951009\n",
      "Epoch 9 training time: 768.90 seconds\n",
      "\n",
      "Test set: Average loss: 1.7129, Accuracy: 61/118 (51.69%)\n",
      "\n",
      "Train Epoch: 10 [0/2239 (0%)]\tLoss: 0.764893\n",
      "Train Epoch: 10 [320/2239 (14%)]\tLoss: 0.798598\n",
      "Train Epoch: 10 [640/2239 (29%)]\tLoss: 0.941282\n",
      "Train Epoch: 10 [960/2239 (43%)]\tLoss: 0.898790\n",
      "Train Epoch: 10 [1280/2239 (57%)]\tLoss: 0.703048\n",
      "Train Epoch: 10 [1600/2239 (71%)]\tLoss: 0.857921\n",
      "Train Epoch: 10 [1920/2239 (86%)]\tLoss: 0.819446\n",
      "Epoch 10 training time: 761.42 seconds\n",
      "\n",
      "Test set: Average loss: 1.7351, Accuracy: 55/118 (46.61%)\n",
      "\n",
      "Train Epoch: 11 [0/2239 (0%)]\tLoss: 0.961882\n",
      "Train Epoch: 11 [320/2239 (14%)]\tLoss: 1.152813\n",
      "Train Epoch: 11 [640/2239 (29%)]\tLoss: 0.526778\n",
      "Train Epoch: 11 [960/2239 (43%)]\tLoss: 0.844365\n",
      "Train Epoch: 11 [1280/2239 (57%)]\tLoss: 0.897413\n",
      "Train Epoch: 11 [1600/2239 (71%)]\tLoss: 0.698298\n",
      "Train Epoch: 11 [1920/2239 (86%)]\tLoss: 1.057300\n",
      "Epoch 11 training time: 769.64 seconds\n",
      "\n",
      "Test set: Average loss: 1.9220, Accuracy: 61/118 (51.69%)\n",
      "\n",
      "Train Epoch: 12 [0/2239 (0%)]\tLoss: 0.695921\n",
      "Train Epoch: 12 [320/2239 (14%)]\tLoss: 0.881412\n",
      "Train Epoch: 12 [640/2239 (29%)]\tLoss: 0.765749\n",
      "Train Epoch: 12 [960/2239 (43%)]\tLoss: 0.460681\n",
      "Train Epoch: 12 [1280/2239 (57%)]\tLoss: 1.035092\n",
      "Train Epoch: 12 [1600/2239 (71%)]\tLoss: 0.731136\n",
      "Train Epoch: 12 [1920/2239 (86%)]\tLoss: 0.538516\n",
      "Epoch 12 training time: 765.01 seconds\n",
      "\n",
      "Test set: Average loss: 1.6958, Accuracy: 63/118 (53.39%)\n",
      "\n",
      "Train Epoch: 13 [0/2239 (0%)]\tLoss: 0.704049\n",
      "Train Epoch: 13 [320/2239 (14%)]\tLoss: 0.852504\n",
      "Train Epoch: 13 [640/2239 (29%)]\tLoss: 0.767193\n",
      "Train Epoch: 13 [960/2239 (43%)]\tLoss: 0.841123\n",
      "Train Epoch: 13 [1280/2239 (57%)]\tLoss: 0.499812\n",
      "Train Epoch: 13 [1600/2239 (71%)]\tLoss: 0.670558\n",
      "Train Epoch: 13 [1920/2239 (86%)]\tLoss: 0.694607\n",
      "Epoch 13 training time: 766.00 seconds\n",
      "\n",
      "Test set: Average loss: 1.6387, Accuracy: 64/118 (54.24%)\n",
      "\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Train Epoch: 14 [0/2239 (0%)]\tLoss: 0.526330\n",
      "Train Epoch: 14 [320/2239 (14%)]\tLoss: 0.688513\n",
      "Train Epoch: 14 [640/2239 (29%)]\tLoss: 0.419208\n",
      "Train Epoch: 14 [960/2239 (43%)]\tLoss: 0.532502\n",
      "Train Epoch: 14 [1280/2239 (57%)]\tLoss: 0.628232\n",
      "Train Epoch: 14 [1600/2239 (71%)]\tLoss: 0.564060\n",
      "Train Epoch: 14 [1920/2239 (86%)]\tLoss: 0.462451\n",
      "Epoch 14 training time: 753.13 seconds\n",
      "\n",
      "Test set: Average loss: 1.1654, Accuracy: 74/118 (62.71%)\n",
      "\n",
      "Train Epoch: 15 [0/2239 (0%)]\tLoss: 0.724592\n",
      "Train Epoch: 15 [320/2239 (14%)]\tLoss: 0.591586\n",
      "Train Epoch: 15 [640/2239 (29%)]\tLoss: 0.670561\n",
      "Train Epoch: 15 [960/2239 (43%)]\tLoss: 0.642303\n",
      "Train Epoch: 15 [1280/2239 (57%)]\tLoss: 0.354942\n",
      "Train Epoch: 15 [1600/2239 (71%)]\tLoss: 0.661710\n",
      "Train Epoch: 15 [1920/2239 (86%)]\tLoss: 0.248026\n",
      "Epoch 15 training time: 763.57 seconds\n",
      "\n",
      "Test set: Average loss: 1.2742, Accuracy: 69/118 (58.47%)\n",
      "\n",
      "Train Epoch: 16 [0/2239 (0%)]\tLoss: 0.680288\n",
      "Train Epoch: 16 [320/2239 (14%)]\tLoss: 0.669618\n",
      "Train Epoch: 16 [640/2239 (29%)]\tLoss: 0.536923\n",
      "Train Epoch: 16 [960/2239 (43%)]\tLoss: 0.628269\n",
      "Train Epoch: 16 [1280/2239 (57%)]\tLoss: 0.422902\n",
      "Train Epoch: 16 [1600/2239 (71%)]\tLoss: 0.585284\n",
      "Train Epoch: 16 [1920/2239 (86%)]\tLoss: 0.528261\n",
      "Epoch 16 training time: 766.54 seconds\n",
      "\n",
      "Test set: Average loss: 1.3650, Accuracy: 69/118 (58.47%)\n",
      "\n",
      "Train Epoch: 17 [0/2239 (0%)]\tLoss: 0.361928\n",
      "Train Epoch: 17 [320/2239 (14%)]\tLoss: 0.658442\n",
      "Train Epoch: 17 [640/2239 (29%)]\tLoss: 0.598112\n",
      "Train Epoch: 17 [960/2239 (43%)]\tLoss: 0.774745\n",
      "Train Epoch: 17 [1280/2239 (57%)]\tLoss: 0.582505\n",
      "Train Epoch: 17 [1600/2239 (71%)]\tLoss: 0.656697\n",
      "Train Epoch: 17 [1920/2239 (86%)]\tLoss: 0.646635\n",
      "Epoch 17 training time: 765.52 seconds\n",
      "\n",
      "Test set: Average loss: 1.4748, Accuracy: 74/118 (62.71%)\n",
      "\n",
      "Train Epoch: 18 [0/2239 (0%)]\tLoss: 0.865272\n",
      "Train Epoch: 18 [320/2239 (14%)]\tLoss: 0.372028\n",
      "Train Epoch: 18 [640/2239 (29%)]\tLoss: 0.540302\n",
      "Train Epoch: 18 [960/2239 (43%)]\tLoss: 0.531395\n",
      "Train Epoch: 18 [1280/2239 (57%)]\tLoss: 0.592180\n",
      "Train Epoch: 18 [1600/2239 (71%)]\tLoss: 0.652094\n",
      "Train Epoch: 18 [1920/2239 (86%)]\tLoss: 0.233070\n",
      "Epoch 18 training time: 768.69 seconds\n",
      "\n",
      "Test set: Average loss: 1.4395, Accuracy: 71/118 (60.17%)\n",
      "\n",
      "Train Epoch: 19 [0/2239 (0%)]\tLoss: 0.335693\n",
      "Train Epoch: 19 [320/2239 (14%)]\tLoss: 0.422418\n",
      "Train Epoch: 19 [640/2239 (29%)]\tLoss: 0.749164\n",
      "Train Epoch: 19 [960/2239 (43%)]\tLoss: 0.779757\n",
      "Train Epoch: 19 [1280/2239 (57%)]\tLoss: 0.786496\n",
      "Train Epoch: 19 [1600/2239 (71%)]\tLoss: 0.911487\n",
      "Train Epoch: 19 [1920/2239 (86%)]\tLoss: 0.767810\n",
      "Epoch 19 training time: 767.11 seconds\n",
      "\n",
      "Test set: Average loss: 1.4611, Accuracy: 68/118 (57.63%)\n",
      "\n",
      "Train Epoch: 20 [0/2239 (0%)]\tLoss: 0.528533\n",
      "Train Epoch: 20 [320/2239 (14%)]\tLoss: 0.570754\n",
      "Train Epoch: 20 [640/2239 (29%)]\tLoss: 0.332521\n",
      "Train Epoch: 20 [960/2239 (43%)]\tLoss: 0.476413\n",
      "Train Epoch: 20 [1280/2239 (57%)]\tLoss: 0.408387\n",
      "Train Epoch: 20 [1600/2239 (71%)]\tLoss: 0.630140\n",
      "Train Epoch: 20 [1920/2239 (86%)]\tLoss: 0.416353\n",
      "Epoch 20 training time: 763.98 seconds\n",
      "\n",
      "Test set: Average loss: 1.4148, Accuracy: 70/118 (59.32%)\n",
      "\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Train Epoch: 21 [0/2239 (0%)]\tLoss: 0.248509\n",
      "Train Epoch: 21 [320/2239 (14%)]\tLoss: 0.512356\n",
      "Train Epoch: 21 [640/2239 (29%)]\tLoss: 0.396017\n",
      "Train Epoch: 21 [960/2239 (43%)]\tLoss: 0.549415\n",
      "Train Epoch: 21 [1280/2239 (57%)]\tLoss: 0.675399\n",
      "Train Epoch: 21 [1600/2239 (71%)]\tLoss: 0.416225\n",
      "Train Epoch: 21 [1920/2239 (86%)]\tLoss: 0.504183\n",
      "Epoch 21 training time: 766.70 seconds\n",
      "\n",
      "Test set: Average loss: 1.2834, Accuracy: 75/118 (63.56%)\n",
      "\n",
      "Train Epoch: 22 [0/2239 (0%)]\tLoss: 0.471418\n",
      "Train Epoch: 22 [320/2239 (14%)]\tLoss: 0.278644\n",
      "Train Epoch: 22 [640/2239 (29%)]\tLoss: 0.720023\n",
      "Train Epoch: 22 [960/2239 (43%)]\tLoss: 0.418256\n",
      "Train Epoch: 22 [1280/2239 (57%)]\tLoss: 0.434997\n",
      "Train Epoch: 22 [1600/2239 (71%)]\tLoss: 0.358497\n",
      "Train Epoch: 22 [1920/2239 (86%)]\tLoss: 0.507744\n",
      "Epoch 22 training time: 765.62 seconds\n",
      "\n",
      "Test set: Average loss: 1.2498, Accuracy: 73/118 (61.86%)\n",
      "\n",
      "Train Epoch: 23 [0/2239 (0%)]\tLoss: 0.496312\n",
      "Train Epoch: 23 [320/2239 (14%)]\tLoss: 0.453449\n",
      "Train Epoch: 23 [640/2239 (29%)]\tLoss: 0.388253\n",
      "Train Epoch: 23 [960/2239 (43%)]\tLoss: 0.394251\n",
      "Train Epoch: 23 [1280/2239 (57%)]\tLoss: 0.595075\n",
      "Train Epoch: 23 [1600/2239 (71%)]\tLoss: 0.333995\n",
      "Train Epoch: 23 [1920/2239 (86%)]\tLoss: 0.568936\n",
      "Epoch 23 training time: 763.02 seconds\n",
      "\n",
      "Test set: Average loss: 1.3593, Accuracy: 69/118 (58.47%)\n",
      "\n",
      "Train Epoch: 24 [0/2239 (0%)]\tLoss: 0.447156\n",
      "Train Epoch: 24 [320/2239 (14%)]\tLoss: 0.546944\n",
      "Train Epoch: 24 [640/2239 (29%)]\tLoss: 0.674864\n",
      "Train Epoch: 24 [960/2239 (43%)]\tLoss: 0.528052\n",
      "Train Epoch: 24 [1280/2239 (57%)]\tLoss: 0.899851\n",
      "Train Epoch: 24 [1600/2239 (71%)]\tLoss: 0.453301\n",
      "Train Epoch: 24 [1920/2239 (86%)]\tLoss: 0.764597\n",
      "Epoch 24 training time: 766.27 seconds\n",
      "\n",
      "Test set: Average loss: 1.3900, Accuracy: 72/118 (61.02%)\n",
      "\n",
      "Train Epoch: 25 [0/2239 (0%)]\tLoss: 0.530347\n",
      "Train Epoch: 25 [320/2239 (14%)]\tLoss: 0.554804\n",
      "Train Epoch: 25 [640/2239 (29%)]\tLoss: 0.383084\n",
      "Train Epoch: 25 [960/2239 (43%)]\tLoss: 0.453668\n",
      "Train Epoch: 25 [1280/2239 (57%)]\tLoss: 0.504299\n",
      "Train Epoch: 25 [1600/2239 (71%)]\tLoss: 0.653673\n",
      "Train Epoch: 25 [1920/2239 (86%)]\tLoss: 0.224358\n",
      "Epoch 25 training time: 765.46 seconds\n",
      "\n",
      "Test set: Average loss: 1.2494, Accuracy: 78/118 (66.10%)\n",
      "\n",
      "Train Epoch: 26 [0/2239 (0%)]\tLoss: 0.769272\n",
      "Train Epoch: 26 [320/2239 (14%)]\tLoss: 0.371103\n",
      "Train Epoch: 26 [640/2239 (29%)]\tLoss: 0.401364\n",
      "Train Epoch: 26 [960/2239 (43%)]\tLoss: 0.328544\n",
      "Train Epoch: 26 [1280/2239 (57%)]\tLoss: 0.400968\n",
      "Train Epoch: 26 [1600/2239 (71%)]\tLoss: 0.498695\n",
      "Train Epoch: 26 [1920/2239 (86%)]\tLoss: 0.506985\n",
      "Epoch 26 training time: 765.22 seconds\n",
      "\n",
      "Test set: Average loss: 1.3178, Accuracy: 73/118 (61.86%)\n",
      "\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Train Epoch: 27 [0/2239 (0%)]\tLoss: 0.528145\n",
      "Train Epoch: 27 [320/2239 (14%)]\tLoss: 0.462014\n",
      "Train Epoch: 27 [640/2239 (29%)]\tLoss: 0.433161\n",
      "Train Epoch: 27 [960/2239 (43%)]\tLoss: 0.353191\n",
      "Train Epoch: 27 [1280/2239 (57%)]\tLoss: 0.570846\n",
      "Train Epoch: 27 [1600/2239 (71%)]\tLoss: 0.486864\n",
      "Train Epoch: 27 [1920/2239 (86%)]\tLoss: 0.541428\n",
      "Epoch 27 training time: 766.70 seconds\n",
      "\n",
      "Test set: Average loss: 1.2411, Accuracy: 74/118 (62.71%)\n",
      "\n",
      "Train Epoch: 28 [0/2239 (0%)]\tLoss: 0.342788\n",
      "Train Epoch: 28 [320/2239 (14%)]\tLoss: 0.619231\n",
      "Train Epoch: 28 [640/2239 (29%)]\tLoss: 0.473229\n",
      "Train Epoch: 28 [960/2239 (43%)]\tLoss: 0.504345\n",
      "Train Epoch: 28 [1280/2239 (57%)]\tLoss: 0.305628\n",
      "Train Epoch: 28 [1600/2239 (71%)]\tLoss: 0.395626\n",
      "Train Epoch: 28 [1920/2239 (86%)]\tLoss: 0.968461\n",
      "Epoch 28 training time: 765.71 seconds\n",
      "\n",
      "Test set: Average loss: 1.2840, Accuracy: 69/118 (58.47%)\n",
      "\n",
      "Train Epoch: 29 [0/2239 (0%)]\tLoss: 0.696452\n",
      "Train Epoch: 29 [320/2239 (14%)]\tLoss: 0.681738\n",
      "Train Epoch: 29 [640/2239 (29%)]\tLoss: 0.395223\n",
      "Train Epoch: 29 [960/2239 (43%)]\tLoss: 0.553708\n",
      "Train Epoch: 29 [1280/2239 (57%)]\tLoss: 0.406719\n",
      "Train Epoch: 29 [1600/2239 (71%)]\tLoss: 0.444978\n",
      "Train Epoch: 29 [1920/2239 (86%)]\tLoss: 0.453711\n",
      "Epoch 29 training time: 755.13 seconds\n",
      "\n",
      "Test set: Average loss: 1.4418, Accuracy: 71/118 (60.17%)\n",
      "\n",
      "Train Epoch: 30 [0/2239 (0%)]\tLoss: 0.949304\n",
      "Train Epoch: 30 [320/2239 (14%)]\tLoss: 0.491864\n",
      "Train Epoch: 30 [640/2239 (29%)]\tLoss: 0.617144\n",
      "Train Epoch: 30 [960/2239 (43%)]\tLoss: 0.265205\n",
      "Train Epoch: 30 [1280/2239 (57%)]\tLoss: 0.668261\n",
      "Train Epoch: 30 [1600/2239 (71%)]\tLoss: 0.594536\n",
      "Train Epoch: 30 [1920/2239 (86%)]\tLoss: 0.267773\n",
      "Epoch 30 training time: 768.28 seconds\n",
      "\n",
      "Test set: Average loss: 1.4646, Accuracy: 71/118 (60.17%)\n",
      "\n",
      "Train Epoch: 31 [0/2239 (0%)]\tLoss: 0.563574\n",
      "Train Epoch: 31 [320/2239 (14%)]\tLoss: 0.434205\n",
      "Train Epoch: 31 [640/2239 (29%)]\tLoss: 0.410056\n",
      "Train Epoch: 31 [960/2239 (43%)]\tLoss: 0.292249\n",
      "Train Epoch: 31 [1280/2239 (57%)]\tLoss: 0.384031\n",
      "Train Epoch: 31 [1600/2239 (71%)]\tLoss: 0.476331\n",
      "Train Epoch: 31 [1920/2239 (86%)]\tLoss: 0.625666\n",
      "Epoch 31 training time: 790.85 seconds\n",
      "\n",
      "Test set: Average loss: 1.4436, Accuracy: 70/118 (59.32%)\n",
      "\n",
      "Train Epoch: 32 [0/2239 (0%)]\tLoss: 0.567232\n",
      "Train Epoch: 32 [320/2239 (14%)]\tLoss: 0.403556\n",
      "Train Epoch: 32 [640/2239 (29%)]\tLoss: 0.551882\n",
      "Train Epoch: 32 [960/2239 (43%)]\tLoss: 0.509968\n",
      "Train Epoch: 32 [1280/2239 (57%)]\tLoss: 0.445563\n",
      "Train Epoch: 32 [1600/2239 (71%)]\tLoss: 0.239944\n",
      "Train Epoch: 32 [1920/2239 (86%)]\tLoss: 0.637258\n",
      "Epoch 32 training time: 946.84 seconds\n",
      "\n",
      "Test set: Average loss: 1.3529, Accuracy: 73/118 (61.86%)\n",
      "\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Train Epoch: 33 [0/2239 (0%)]\tLoss: 0.470412\n",
      "Train Epoch: 33 [320/2239 (14%)]\tLoss: 0.538881\n",
      "Train Epoch: 33 [640/2239 (29%)]\tLoss: 0.602885\n",
      "Train Epoch: 33 [960/2239 (43%)]\tLoss: 0.510327\n",
      "Train Epoch: 33 [1280/2239 (57%)]\tLoss: 0.484984\n",
      "Train Epoch: 33 [1600/2239 (71%)]\tLoss: 0.647702\n",
      "Train Epoch: 33 [1920/2239 (86%)]\tLoss: 0.328797\n",
      "Epoch 33 training time: 936.66 seconds\n",
      "\n",
      "Test set: Average loss: 1.3353, Accuracy: 76/118 (64.41%)\n",
      "\n",
      "Train Epoch: 34 [0/2239 (0%)]\tLoss: 0.515489\n",
      "Train Epoch: 34 [320/2239 (14%)]\tLoss: 0.578983\n",
      "Train Epoch: 34 [640/2239 (29%)]\tLoss: 0.398032\n",
      "Train Epoch: 34 [960/2239 (43%)]\tLoss: 0.439726\n",
      "Train Epoch: 34 [1280/2239 (57%)]\tLoss: 0.438206\n",
      "Train Epoch: 34 [1600/2239 (71%)]\tLoss: 0.302232\n",
      "Train Epoch: 34 [1920/2239 (86%)]\tLoss: 0.569844\n",
      "Epoch 34 training time: 911.37 seconds\n",
      "\n",
      "Test set: Average loss: 1.2741, Accuracy: 76/118 (64.41%)\n",
      "\n",
      "Train Epoch: 35 [0/2239 (0%)]\tLoss: 0.694104\n",
      "Train Epoch: 35 [320/2239 (14%)]\tLoss: 0.293739\n",
      "Train Epoch: 35 [640/2239 (29%)]\tLoss: 0.706892\n",
      "Train Epoch: 35 [960/2239 (43%)]\tLoss: 0.463656\n",
      "Train Epoch: 35 [1280/2239 (57%)]\tLoss: 0.695260\n",
      "Train Epoch: 35 [1600/2239 (71%)]\tLoss: 0.434245\n",
      "Train Epoch: 35 [1920/2239 (86%)]\tLoss: 0.606562\n",
      "Epoch 35 training time: 918.36 seconds\n",
      "\n",
      "Test set: Average loss: 1.1648, Accuracy: 76/118 (64.41%)\n",
      "\n",
      "Train Epoch: 36 [0/2239 (0%)]\tLoss: 0.427509\n",
      "Train Epoch: 36 [320/2239 (14%)]\tLoss: 0.448055\n",
      "Train Epoch: 36 [640/2239 (29%)]\tLoss: 0.388050\n",
      "Train Epoch: 36 [960/2239 (43%)]\tLoss: 0.389926\n",
      "Train Epoch: 36 [1280/2239 (57%)]\tLoss: 0.339737\n",
      "Train Epoch: 36 [1600/2239 (71%)]\tLoss: 0.398139\n",
      "Train Epoch: 36 [1920/2239 (86%)]\tLoss: 0.506450\n",
      "Epoch 36 training time: 942.57 seconds\n",
      "\n",
      "Test set: Average loss: 1.3744, Accuracy: 75/118 (63.56%)\n",
      "\n",
      "Train Epoch: 37 [0/2239 (0%)]\tLoss: 0.703628\n",
      "Train Epoch: 37 [320/2239 (14%)]\tLoss: 0.238376\n",
      "Train Epoch: 37 [640/2239 (29%)]\tLoss: 0.396286\n",
      "Train Epoch: 37 [960/2239 (43%)]\tLoss: 0.359284\n",
      "Train Epoch: 37 [1280/2239 (57%)]\tLoss: 0.736059\n",
      "Train Epoch: 37 [1600/2239 (71%)]\tLoss: 0.286020\n",
      "Train Epoch: 37 [1920/2239 (86%)]\tLoss: 0.357162\n",
      "Epoch 37 training time: 1512.65 seconds\n",
      "\n",
      "Test set: Average loss: 1.4415, Accuracy: 67/118 (56.78%)\n",
      "\n",
      "Train Epoch: 38 [0/2239 (0%)]\tLoss: 0.289975\n",
      "Train Epoch: 38 [320/2239 (14%)]\tLoss: 0.367821\n",
      "Train Epoch: 38 [640/2239 (29%)]\tLoss: 0.309817\n",
      "Train Epoch: 38 [960/2239 (43%)]\tLoss: 0.676936\n",
      "Train Epoch: 38 [1280/2239 (57%)]\tLoss: 0.455388\n",
      "Train Epoch: 38 [1600/2239 (71%)]\tLoss: 0.587751\n",
      "Train Epoch: 38 [1920/2239 (86%)]\tLoss: 0.528796\n",
      "Epoch 38 training time: 934.87 seconds\n",
      "\n",
      "Test set: Average loss: 1.3037, Accuracy: 70/118 (59.32%)\n",
      "\n",
      "Train Epoch: 39 [0/2239 (0%)]\tLoss: 0.437165\n",
      "Train Epoch: 39 [320/2239 (14%)]\tLoss: 0.587966\n",
      "Train Epoch: 39 [640/2239 (29%)]\tLoss: 0.495840\n",
      "Train Epoch: 39 [960/2239 (43%)]\tLoss: 0.458302\n",
      "Train Epoch: 39 [1280/2239 (57%)]\tLoss: 0.373038\n",
      "Train Epoch: 39 [1600/2239 (71%)]\tLoss: 0.716104\n",
      "Train Epoch: 39 [1920/2239 (86%)]\tLoss: 0.588868\n",
      "Epoch 39 training time: 939.85 seconds\n",
      "\n",
      "Test set: Average loss: 1.3209, Accuracy: 73/118 (61.86%)\n",
      "\n",
      "Train Epoch: 40 [0/2239 (0%)]\tLoss: 0.531786\n",
      "Train Epoch: 40 [320/2239 (14%)]\tLoss: 0.634013\n",
      "Train Epoch: 40 [640/2239 (29%)]\tLoss: 0.541284\n",
      "Train Epoch: 40 [960/2239 (43%)]\tLoss: 0.307448\n",
      "Train Epoch: 40 [1280/2239 (57%)]\tLoss: 0.366361\n",
      "Train Epoch: 40 [1600/2239 (71%)]\tLoss: 0.566018\n",
      "Train Epoch: 40 [1920/2239 (86%)]\tLoss: 0.277972\n",
      "Epoch 40 training time: 969.37 seconds\n",
      "\n",
      "Test set: Average loss: 1.4153, Accuracy: 72/118 (61.02%)\n",
      "\n",
      "Train Epoch: 41 [0/2239 (0%)]\tLoss: 0.394636\n",
      "Train Epoch: 41 [320/2239 (14%)]\tLoss: 0.630360\n",
      "Train Epoch: 41 [640/2239 (29%)]\tLoss: 0.456639\n",
      "Train Epoch: 41 [960/2239 (43%)]\tLoss: 0.497100\n",
      "Train Epoch: 41 [1280/2239 (57%)]\tLoss: 0.454007\n",
      "Train Epoch: 41 [1600/2239 (71%)]\tLoss: 0.339004\n",
      "Train Epoch: 41 [1920/2239 (86%)]\tLoss: 0.562499\n",
      "Epoch 41 training time: 1015.18 seconds\n",
      "\n",
      "Test set: Average loss: 1.4301, Accuracy: 73/118 (61.86%)\n",
      "\n",
      "Train Epoch: 42 [0/2239 (0%)]\tLoss: 0.678080\n",
      "Train Epoch: 42 [320/2239 (14%)]\tLoss: 0.410902\n",
      "Train Epoch: 42 [640/2239 (29%)]\tLoss: 0.372786\n",
      "Train Epoch: 42 [960/2239 (43%)]\tLoss: 0.590192\n",
      "Train Epoch: 42 [1280/2239 (57%)]\tLoss: 0.371217\n",
      "Train Epoch: 42 [1600/2239 (71%)]\tLoss: 0.451959\n",
      "Train Epoch: 42 [1920/2239 (86%)]\tLoss: 0.366736\n",
      "Epoch 42 training time: 876.32 seconds\n",
      "\n",
      "Test set: Average loss: 1.3225, Accuracy: 71/118 (60.17%)\n",
      "\n",
      "Train Epoch: 43 [0/2239 (0%)]\tLoss: 0.603069\n",
      "Train Epoch: 43 [320/2239 (14%)]\tLoss: 0.606349\n",
      "Train Epoch: 43 [640/2239 (29%)]\tLoss: 0.651672\n",
      "Train Epoch: 43 [960/2239 (43%)]\tLoss: 0.564897\n",
      "Train Epoch: 43 [1280/2239 (57%)]\tLoss: 0.912288\n",
      "Train Epoch: 43 [1600/2239 (71%)]\tLoss: 0.298076\n",
      "Train Epoch: 43 [1920/2239 (86%)]\tLoss: 0.855815\n",
      "Epoch 43 training time: 819.38 seconds\n",
      "\n",
      "Test set: Average loss: 1.4097, Accuracy: 68/118 (57.63%)\n",
      "\n",
      "Train Epoch: 44 [0/2239 (0%)]\tLoss: 0.340493\n",
      "Train Epoch: 44 [320/2239 (14%)]\tLoss: 0.660668\n",
      "Train Epoch: 44 [640/2239 (29%)]\tLoss: 0.528106\n",
      "Train Epoch: 44 [960/2239 (43%)]\tLoss: 0.453330\n",
      "Train Epoch: 44 [1280/2239 (57%)]\tLoss: 0.440475\n",
      "Train Epoch: 44 [1600/2239 (71%)]\tLoss: 0.667103\n",
      "Train Epoch: 44 [1920/2239 (86%)]\tLoss: 0.556290\n",
      "Epoch 44 training time: 1445.18 seconds\n",
      "\n",
      "Test set: Average loss: 1.2734, Accuracy: 77/118 (65.25%)\n",
      "\n",
      "Train Epoch: 45 [0/2239 (0%)]\tLoss: 0.602967\n",
      "Train Epoch: 45 [320/2239 (14%)]\tLoss: 0.458236\n",
      "Train Epoch: 45 [640/2239 (29%)]\tLoss: 0.487937\n",
      "Train Epoch: 45 [960/2239 (43%)]\tLoss: 0.418550\n",
      "Train Epoch: 45 [1280/2239 (57%)]\tLoss: 0.337432\n",
      "Train Epoch: 45 [1600/2239 (71%)]\tLoss: 0.300489\n",
      "Train Epoch: 45 [1920/2239 (86%)]\tLoss: 0.548058\n",
      "Epoch 45 training time: 799.61 seconds\n",
      "\n",
      "Test set: Average loss: 1.2742, Accuracy: 70/118 (59.32%)\n",
      "\n",
      "Train Epoch: 46 [0/2239 (0%)]\tLoss: 0.330560\n",
      "Train Epoch: 46 [320/2239 (14%)]\tLoss: 0.432276\n",
      "Train Epoch: 46 [640/2239 (29%)]\tLoss: 0.436923\n",
      "Train Epoch: 46 [960/2239 (43%)]\tLoss: 0.767731\n",
      "Train Epoch: 46 [1280/2239 (57%)]\tLoss: 0.352503\n",
      "Train Epoch: 46 [1600/2239 (71%)]\tLoss: 0.350371\n",
      "Train Epoch: 46 [1920/2239 (86%)]\tLoss: 0.580952\n",
      "Epoch 46 training time: 747.23 seconds\n",
      "\n",
      "Test set: Average loss: 1.2452, Accuracy: 74/118 (62.71%)\n",
      "\n",
      "Train Epoch: 47 [0/2239 (0%)]\tLoss: 0.376636\n",
      "Train Epoch: 47 [320/2239 (14%)]\tLoss: 0.492888\n",
      "Train Epoch: 47 [640/2239 (29%)]\tLoss: 0.632988\n",
      "Train Epoch: 47 [960/2239 (43%)]\tLoss: 0.412181\n",
      "Train Epoch: 47 [1280/2239 (57%)]\tLoss: 0.455558\n",
      "Train Epoch: 47 [1600/2239 (71%)]\tLoss: 0.666562\n",
      "Train Epoch: 47 [1920/2239 (86%)]\tLoss: 0.350956\n",
      "Epoch 47 training time: 753.85 seconds\n",
      "\n",
      "Test set: Average loss: 1.3719, Accuracy: 71/118 (60.17%)\n",
      "\n",
      "Train Epoch: 48 [0/2239 (0%)]\tLoss: 0.573108\n",
      "Train Epoch: 48 [320/2239 (14%)]\tLoss: 0.371605\n",
      "Train Epoch: 48 [640/2239 (29%)]\tLoss: 0.605563\n",
      "Train Epoch: 48 [960/2239 (43%)]\tLoss: 0.571256\n",
      "Train Epoch: 48 [1280/2239 (57%)]\tLoss: 0.446020\n",
      "Train Epoch: 48 [1600/2239 (71%)]\tLoss: 0.656761\n",
      "Train Epoch: 48 [1920/2239 (86%)]\tLoss: 0.816860\n",
      "Epoch 48 training time: 644.95 seconds\n",
      "\n",
      "Test set: Average loss: 1.3838, Accuracy: 70/118 (59.32%)\n",
      "\n",
      "Train Epoch: 49 [0/2239 (0%)]\tLoss: 0.502045\n",
      "Train Epoch: 49 [320/2239 (14%)]\tLoss: 0.483104\n",
      "Train Epoch: 49 [640/2239 (29%)]\tLoss: 0.414675\n",
      "Train Epoch: 49 [960/2239 (43%)]\tLoss: 0.441322\n",
      "Train Epoch: 49 [1280/2239 (57%)]\tLoss: 0.342792\n",
      "Train Epoch: 49 [1600/2239 (71%)]\tLoss: 0.512811\n",
      "Train Epoch: 49 [1920/2239 (86%)]\tLoss: 0.362769\n",
      "Epoch 49 training time: 593.74 seconds\n",
      "\n",
      "Test set: Average loss: 1.4467, Accuracy: 69/118 (58.47%)\n",
      "\n",
      "Train Epoch: 50 [0/2239 (0%)]\tLoss: 0.443312\n",
      "Train Epoch: 50 [320/2239 (14%)]\tLoss: 0.335634\n",
      "Train Epoch: 50 [640/2239 (29%)]\tLoss: 0.487230\n",
      "Train Epoch: 50 [960/2239 (43%)]\tLoss: 0.372972\n",
      "Train Epoch: 50 [1280/2239 (57%)]\tLoss: 0.561137\n",
      "Train Epoch: 50 [1600/2239 (71%)]\tLoss: 0.480128\n",
      "Train Epoch: 50 [1920/2239 (86%)]\tLoss: 0.448159\n",
      "Epoch 50 training time: 594.47 seconds\n",
      "\n",
      "Test set: Average loss: 1.3181, Accuracy: 76/118 (64.41%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import time\n",
    "import torchdiffeq\n",
    "\n",
    "class TaggedModule(nn.Module):\n",
    "    def __init__(self, tag):\n",
    "        super().__init__()\n",
    "        self.tag = tag\n",
    "\n",
    "class ResNet50(TaggedModule):\n",
    "    def __init__(self):\n",
    "        super().__init__(tag=\"non_ode\")\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1], nn.Dropout(0.5))  # Add Dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ODEFunc(TaggedModule):\n",
    "    def __init__(self):\n",
    "        super().__init__(tag=\"ode\")\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2048, 2048)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return self.net(y)\n",
    "\n",
    "class AugmentedODEFunc(TaggedModule):\n",
    "    def __init__(self, func):\n",
    "        super().__init__(tag=\"ode\")\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, t, state):\n",
    "        y, adj_y = state[:, :2048], state[:, 2048:]\n",
    "        with torch.set_grad_enabled(True):\n",
    "            y.requires_grad_(True)\n",
    "            dy = self.func(t, y)\n",
    "            adj_dy = torch.autograd.grad(dy, y, adj_y, retain_graph=True)[0]\n",
    "        return torch.cat([dy, adj_dy], dim=1)\n",
    "\n",
    "class ODEModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ODEModel, self).__init__()\n",
    "        self.resnet = ResNet50()\n",
    "        self.ode_func = ODEFunc()\n",
    "        self.aug_ode_func = AugmentedODEFunc(self.ode_func)\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "        self.classifier.tag = \"non_ode\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        if self.training:\n",
    "            aug_state = torch.cat([x, torch.zeros_like(x)], dim=1)\n",
    "            t = torch.tensor([0., 1.]).to(x.device)\n",
    "            aug_state = torchdiffeq.odeint(self.aug_ode_func, aug_state, t, method='dopri5')\n",
    "            y1, adj_y1 = aug_state[-1, :, :2048], aug_state[-1, :, 2048:]\n",
    "            output = self.classifier(y1)\n",
    "            return output, y1, adj_y1\n",
    "        else:\n",
    "            t = torch.tensor([0., 1.]).to(x.device)\n",
    "            y1 = torchdiffeq.odeint(self.ode_func, x, t, method='dopri5')\n",
    "            output = self.classifier(y1[-1])\n",
    "            return output\n",
    "\n",
    "def get_parameters_by_tag(model, tag):\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, 'tag') and module.tag == tag:\n",
    "            yield from module.parameters()\n",
    "\n",
    "def compute_loss_and_grad(model, data, target):\n",
    "    output, y1, adj_y1 = model(data)\n",
    "    loss = F.cross_entropy(output, target)\n",
    "    grad_output = torch.autograd.grad(loss, y1, create_graph=True)[0]\n",
    "    return loss, grad_output, adj_y1\n",
    "\n",
    "def train(epoch, model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss, grad_output, adj_y1 = compute_loss_and_grad(model, data, target)\n",
    "\n",
    "        ode_params = list(get_parameters_by_tag(model, \"ode\"))\n",
    "        non_ode_params = list(get_parameters_by_tag(model, \"non_ode\"))\n",
    "\n",
    "        non_ode_grads = torch.autograd.grad(loss, non_ode_params, allow_unused=True, create_graph=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param, grad in zip(non_ode_params, non_ode_grads):\n",
    "                if param.requires_grad and grad is not None:\n",
    "                    param.grad = grad\n",
    "\n",
    "            adj_grad = torch.einsum('bi,bj->ij', adj_y1, grad_output)\n",
    "            adj_grad_flat = adj_grad.flatten()\n",
    "\n",
    "            start = 0\n",
    "            for param in ode_params:\n",
    "                if param.requires_grad:\n",
    "                    num_param = param.numel()\n",
    "                    if start + num_param <= adj_grad_flat.numel():\n",
    "                        param_grad = adj_grad_flat[start:start+num_param].view(param.shape)\n",
    "                        param.grad = param_grad\n",
    "                    start += num_param\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    end_time = time.time()\n",
    "    print(f'Epoch {epoch} training time: {end_time - start_time:.2f} seconds')\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({accuracy:.2f}%)\\n')\n",
    "    return test_loss, accuracy\n",
    "\n",
    "def load_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    train_dataset = ImageFolder('../data/Skin_cancer_ISIC/Train', transform=transform)\n",
    "    test_dataset = ImageFolder('../data/Skin_cancer_ISIC/Test', transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    return train_loader, test_loader, len(train_dataset.classes)\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_loader, test_loader, num_classes = load_data()\n",
    "    model = ODEModel(num_classes).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    for epoch in range(1, 51):\n",
    "        train(epoch, model, device, train_loader, optimizer)\n",
    "        test_loss, accuracy = test(model, device, test_loader)\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccf640-4d99-4b6b-a68c-73139358226f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
